{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"../results\"\n",
    "exp = \"oed\"\n",
    "env = \"hyperbolic_temporal_discount\"\n",
    "env = \"location_finding\"\n",
    "env = \"death_process\"\n",
    "# env= \"irt\"\n",
    "# env = \"dugongs\"\n",
    "# env = \"survival\"\n",
    "# env = \"peregrines\"\n",
    "env = \"morals\"\n",
    "# goal = \"direct\"\n",
    "model = \"gpt-4o-boxloop\"\n",
    "seeds = [1,2,3,4,5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(results_dir, env, f\"{goal}_{model}_{exp}_True_{seed}.json\") for seed in seeds]\n",
    "files_no_prior = [os.path.join(results_dir, env, f\"{goal}_{model}_{exp}_False_{seed}.json\") for seed in seeds]\n",
    "regret_files = [os.path.join(results_dir, env, f\"regret_{goal}_{model}_{exp}_True_{seed}.json\") for seed in seeds]\n",
    "regret_files_no_prior = [os.path.join(results_dir, env, f\"regret_{goal}_{model}_{exp}_False_{seed}.json\") for seed in seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in files:\n",
    "    \n",
    "    if not os.path.exists(file):\n",
    "        continue\n",
    "    with open(file, \"r\") as f:\n",
    "        data.append(json.load(f))\n",
    "        \n",
    "data_no_prior = []\n",
    "for file in files_no_prior:\n",
    "    if not os.path.exists(file):\n",
    "        continue\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        data_no_prior.append(json.load(f))\n",
    "regrets = []\n",
    "for file in regret_files:\n",
    "    if not os.path.exists(file):\n",
    "        continue\n",
    "\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        regrets.append(json.load(f))\n",
    "regrets_no_prior = []\n",
    "for file in regret_files_no_prior:\n",
    "    if not os.path.exists(file):\n",
    "        continue\n",
    "    print(file)\n",
    "    with open(file, \"r\") as f:\n",
    "        regrets_no_prior.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_no_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "1 10\n",
      "2 10\n",
      "3 10\n",
      "4 10\n",
      "[[0.3, 0.5, 0.5], [0.7, 0.4, 0.6], [0.2, 0.5, 0.4], [0.6, 0.4, 0.3], [0.4, 0.5, 0.6]]\n",
      "removing\n",
      "0.56\n",
      "0.16257370021008932\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[375], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_score[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(ci_95[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmean_score_no_prior\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(ci_95_no_prior[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "observations = [d['data']['observations'] for d in data]\n",
    "observations_no_prior = [d['data']['observations'] for d in data_no_prior]\n",
    "\n",
    "successes = [d['data']['successes'] for d in data]\n",
    "successes_no_prior = [d['data']['successes'] for d in data_no_prior]\n",
    "for i, s in enumerate(successes):\n",
    "    print(i, sum(s))\n",
    "\n",
    "errs, stds = [], []\n",
    "for d in data:\n",
    "    errs.append([])\n",
    "    stds.append([])\n",
    "    for r in d['data']['results']: \n",
    "        errs[-1].append(r[0][0])\n",
    "        stds[-1].append(r[0][1])\n",
    "print(errs)\n",
    "errs_no_prior, stds_no_prior = [], []\n",
    "for d in data_no_prior:\n",
    "    errs_no_prior.append([])\n",
    "    stds_no_prior.append([])\n",
    "    for r in d['data']['results']: \n",
    "        errs_no_prior[-1].append(r[0][0])\n",
    "        stds_no_prior[-1].append(r[0][1])\n",
    "\n",
    "errs = np.array(errs)\n",
    "stds = np.array(stds)\n",
    "errs_no_prior = np.array(errs_no_prior)\n",
    "stds_no_prior = np.array(stds_no_prior)\n",
    "\n",
    "if env == \"hyperbolic_temporal_discount\" and \"direct\" in goal:\n",
    "    errs = 1 - errs\n",
    "    errs_no_prior = 1 - errs_no_prior\n",
    "if env == \"peregrines\":\n",
    "    errs = errs ** 0.5\n",
    "    errs_no_prior = errs_no_prior ** 0.5 \n",
    "if env == \"survival\":\n",
    "    errs = 1 - errs\n",
    "    errs_no_prior = 1 - errs_no_prior\n",
    "if env == \"morals\":\n",
    "    print(\"removing\")\n",
    "    errs = 1 - errs\n",
    "\n",
    "\n",
    "mean_score = np.mean(errs, axis=0)\n",
    "ci_95 = 1.96 * np.std(errs, axis=0) / np.sqrt(len(errs))\n",
    "mean_score_no_prior = None\n",
    "ci_95_no_prior = None\n",
    "if len(errs_no_prior) > 0:\n",
    "    mean_score_no_prior = np.mean(errs_no_prior, axis=0)\n",
    "    ci_95_no_prior = 1.96 * np.std(errs_no_prior, axis=0) / np.sqrt(len(errs_no_prior))\n",
    "\n",
    "\n",
    "\n",
    "print(mean_score[0])\n",
    "print(ci_95[0])\n",
    "print(mean_score_no_prior[0])\n",
    "print(ci_95_no_prior[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot data as a lineplot with error bands\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# colors = sns.color_palette()\n",
    "\n",
    "# exp_list = [0, 1, 3, 5, 7, 10]\n",
    "\n",
    "# def plot_lines(x_list, mean1, ci1, mean2, ci2, label1, label2, xlabel, ylabel, title, save_path):\n",
    "\n",
    "#     # Plot data\n",
    "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
    "#     colors = sns.color_palette(\"colorblind\")\n",
    "\n",
    "#     ax.plot(x_list, mean1, label=label1, color=colors[0], linestyle='-', linewidth=2)\n",
    "#     ax.fill_between(x_list, np.array(mean1) - np.array(ci1), np.array(mean1) + np.array(ci1), color=colors[0], alpha=0.3)\n",
    "\n",
    "#     if mean2 is not None:\n",
    "#         ax.plot(x_list, mean2, label=label2, color=colors[1], linestyle='--', linewidth=2)\n",
    "#         ax.fill_between(x_list, np.array(mean2) - np.array(ci2), np.array(mean2) + np.array(ci2), color=colors[1], alpha=0.3)\n",
    "\n",
    "#     # Customize the grid, legend, and labels\n",
    "#     ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "#     ax.set_xlabel(xlabel, fontsize=14)\n",
    "#     ax.set_ylabel(ylabel, fontsize=14)\n",
    "#     ax.set_title(title, fontsize=16)\n",
    "\n",
    "#     # Customize the legend\n",
    "#     legend = ax.legend(loc='upper left', frameon=True, framealpha=0.9, fontsize=12)\n",
    "#     frame = legend.get_frame()\n",
    "#     frame.set_color('white')\n",
    "\n",
    "#     #  Ensure x-axis is visible\n",
    "#     ax.xaxis.set_visible(True)\n",
    "#     ax.spines['bottom'].set_visible(True)\n",
    "\n",
    "#     # set x-axis ticks\n",
    "#     ax.set_xticks(exp_list)\n",
    "\n",
    "#     # Set white background\n",
    "#     fig.patch.set_facecolor('white')\n",
    "#     ax.set_facecolor('white')\n",
    "\n",
    "#     # Add horizontal grid lines\n",
    "#     ax.yaxis.grid(True, linestyle='--', linewidth=0.5, color='lightgrey')\n",
    "#     # save the plot\n",
    "#     plt.savefig(save_path)\n",
    "\n",
    "# save_path = os.path.join('../plots', f\"{env}_{goal}_{model}_{exp}_oed_error.png\")\n",
    "# plot_lines(exp_list, mean_score, ci_95, mean_score_no_prior, ci_95_no_prior, \"With Prior\", \"Without Prior\", \"Number of Observations\", \"Error\", \"Error with and without Prior\", save_path)\n",
    "# print(mean_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# print len of queries in data\n",
    "print(len(data[0]['data']['queries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(eigs).shape)\n",
    "# print(np.array(eigs_no_prior).shape)\n",
    "# print(np.array(eig_regrets).shape)\n",
    "# print(np.array(eigregrets_no_prior).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if env == \"irt\":\n",
    "#     #TODO: hacky fix for irt\n",
    "#     observation_nums = list(range(9))\n",
    "# else:\n",
    "#     observation_nums = list(range(10))\n",
    "# eigs = np.array(eigs)\n",
    "# eigs_no_prior = np.array(eigs_no_prior)\n",
    "# # clip eigs at 100, -100\n",
    "# # eigs = np.clip(eigs, -100, 100)\n",
    "# # eigs_no_prior = np.clip(eigs_no_prior, -100, 100)\n",
    "# if len(eigs) > 0:\n",
    "#     mean_eigs = np.mean(eigs, axis=0)\n",
    "#     ci_95_eigs = 1.96 * np.std(eigs, axis=0) / np.sqrt(len(eigs))\n",
    "#     if len(eigs_no_prior) > 0:\n",
    "#         mean_eigs_no_prior = np.mean(eigs_no_prior, axis=0)\n",
    "#         ci_95_eigs_no_prior = 1.96 * np.std(eigs_no_prior, axis=0) / np.sqrt(len(eigs_no_prior))\n",
    "#     else:\n",
    "#         mean_eigs_no_prior = None\n",
    "#         ci_95_eigs_no_prior = None\n",
    "#     save_path = os.path.join('../plots', f\"{env}_{goal}_{model}_{exp}_oed_eigs.png\")\n",
    "#     plot_lines(observation_nums, mean_eigs, ci_95_eigs, mean_eigs_no_prior, ci_95_eigs_no_prior, \"With Prior\", \"Without Prior\", \"Number of Observations\", \"Expected IG\", \"Expected IG with and without Prior\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regret_prior = [regret['eigs_regret'] for regret in regrets]\n",
    "# regret_no_prior = [regret['eigs_regret'] for regret in regrets_no_prior]\n",
    "# regret_prior = eig_regrets\n",
    "# regret_no_prior = eigregrets_no_prior\n",
    "# if env == \"irt\":\n",
    "#     #TODO: hacky fix for irt\n",
    "#     observation_nums = list(range(9))\n",
    "# else:\n",
    "#     observation_nums = list(range(10))\n",
    "\n",
    "# mean_regret = None\n",
    "# if len(regret_prior) > 0:\n",
    "#     mean_regret = np.mean(regret_prior, axis=0)\n",
    "#     ci_95_regret = 1.96 * np.std(regret_prior, axis=0) / np.sqrt(len(regret_prior))\n",
    "#     mean_regret_no_prior = None\n",
    "#     if len(regret_no_prior) > 0:\n",
    "#         mean_regret_no_prior = np.mean(regret_no_prior, axis=0)\n",
    "#         ci_95_regret_no_prior = 1.96 * np.std(regret_no_prior, axis=0) / np.sqrt(len(regret_no_prior))\n",
    "#     else:\n",
    "#         mean_regret_no_prior = None\n",
    "#         ci_95_regret_no_prior = None\n",
    "\n",
    "#     save_path = os.path.join('../plots', f\"{env}_{goal}_{model}_{exp}_oed_regret.png\")\n",
    "#     plot_lines(observation_nums, mean_regret, ci_95_regret, mean_regret_no_prior, ci_95_regret_no_prior, \"With Prior\", \"Without Prior\", \"Number of Observations\", \"Expected IG\", \"Expected IG Regret with and without Prior\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56, 0.54, 0.52])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_no_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morals\n",
      "Metric                                   &  \\\\\n",
      "Prior Mean Error at 10 Observations      & 0.52 (0.10222109371357749)\n",
      "Prior Mean Error at 0 Observations       & 0.56 (0.16257370021008932)\n"
     ]
    }
   ],
   "source": [
    "# Metrics to report in the table:\n",
    "print(env)\n",
    "print(f\"{'Metric':<40} &  \\\\\\\\\")\n",
    "# 1. Mean error at 10 observations\n",
    "mean_error_10 = mean_score[-1]\n",
    "ci_95_error_10 = ci_95[-1]\n",
    "print(f\"{'Prior Mean Error at 10 Observations':<40} & {mean_error_10} ({ci_95_error_10})\") \n",
    "# 2. Mean error at 10 observations without prior\n",
    "if mean_score_no_prior is not None:\n",
    "    mean_error_10_no_prior = mean_score_no_prior[-1]\n",
    "    ci_95_error_10_no_prior = ci_95_no_prior[-1]\n",
    "    print(f\"{'No Prior Mean Error at 10 Observations':<40} & {mean_error_10_no_prior} ({ci_95_error_10_no_prior}) \\\\\\\\\")\n",
    "# 3. Mean error at 0 observations\n",
    "mean_error_0 = mean_score[0]\n",
    "ci_95_error_0 = ci_95[0]\n",
    "print(f\"{'Prior Mean Error at 0 Observations':<40} & {mean_error_0} ({ci_95_error_0})\")\n",
    "# 4. Mean error at 0 observations without prior\n",
    "if mean_score_no_prior is not None:\n",
    "    mean_error_0_no_prior = mean_score_no_prior[0]\n",
    "    ci_95_error_0_no_prior = ci_95_no_prior[0]\n",
    "    print(f\"{'No Prior Mean Error at 0 Observations':<40} & {mean_error_0_no_prior} ({ci_95_error_0_no_prior}) \\\\\\\\\")\n",
    "# # 5. Mean regret over all observations\n",
    "# if mean_regret is not None:\n",
    "#     mean_regret_all = np.mean(mean_regret)\n",
    "#     ci_95_regret_all = np.mean(ci_95_regret)\n",
    "#     print(f\"{'Mean Regret over all Observations':<40} & {mean_regret_all} ({ci_95_regret_all})\")\n",
    "# # 6. Mean regret over all observations without prior\n",
    "# if mean_regret_no_prior is not None:\n",
    "#     mean_regret_all_no_prior = np.mean(mean_regret_no_prior)\n",
    "#     ci_95_regret_all_no_prior = np.mean(ci_95_regret_no_prior)\n",
    "#     print(f\"{'No Prior Mean Regret over all Observations':<40} & {mean_regret_all_no_prior} ({ci_95_regret_all_no_prior}) \\\\\\\\\")\n",
    "\n",
    "# # Print the table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed_llms_pymc_openai_updated",
   "language": "python",
   "name": "oed_llms_pymc_openai_updated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
